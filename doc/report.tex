% Very simple template for lab reports. Most common packages are already included.
\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc} % Change according your file encoding
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{float}


%----------------------Definition pour listings------------------------
\lstset{ %
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1 each line 
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
%backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                    % adds a frame around the code
tabsize=2,                        % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
                 % show the filename of files included with \lstinputlisting;
                                % also try caption instead of title
escapeinside={\%*}{*)},         % if you want to add a comment within your code
morekeywords={*,...}            % if you want to add more keywords to the set
}

%opening
\title{Report 5: Chordy, a distributed hash table}
\author{Alexandre Tamborrino}
\date{\today{}}

\begin{document}

\maketitle

\section{Introduction}

In this seminar we have implemented a peer-to-peer distributed hash table (DHT) following the Chord protocol. DHT are important because they allow horizontal scalability of in-memory stored data and fault-tolerance.


\section{Main problems and solutions}

First, we have only implemented the DHT's \textit{ring} without the ability of storing and retrieving data. 

Considering the stabilize operation, each node sends a \textit{request} message to its successor telling it that it wants to know its predecessor. One specific case is when there is only one node in the ring : at the beginning, the successor of the single node is itself and it has no predecessor. Then, it will begin the \textit{stabilizing} procedure: it sends a \textit{request} message to its successor (itself), and then it answers to itself that it has no predecessor, therefore it sends to itself to take itself as a predecessor (\textit{notifying}). Thus, when there is only one node in the system, it has itself as predecessor and successor (in the code, Xkey is equal to Skey).

Moreover, the frequency of this stabilizing operation is every 100 ms for each node. The pros of a frequent stabilizing procedure are that the system sees very rapidly if another node has entered the system (or if a node has crashed), and then it is able to very rapidly re-build a correct ring. The cons are obviously that it induces a network traffic overload. On the other hand, if there is no stabilizing procedure, the ring can not be re-build after additions or deletions of nodes, so it is important to find the right frequency for this operation.

Secondly, we have added a storage mechanism to the DHT implementation. The tricky part is to correctly split the data store when a new node is added to the system. More specifically,  if the node N has a new predecessor P, N should keep the keys-values between (Pkey,Nkey] and give the rest to P. To do this, I have changed the arity of the function split/2 to split/3 as follows: \textit{split(Nkey, Pkey, Store)}. Here is the code:
\begin{lstlisting}[language=erlang]
split(LocalKey, Pkey, Storage) ->
  {Keep, Give} = lists:foldl(fun({Key,Value},{AccSplit1,AccSplit2}) ->
                   case key:between(Key, Pkey, LocalKey) of
                       true ->
                           %% keep for local node
                           {[{Key,Value}|AccSplit1],AccSplit2};
                       false ->
                           {AccSplit1,[{Key,Value}|AccSplit2]}
                   end
                 end, {[],[]}, Storage).
\end{lstlisting}

\section{Evaluation}

In order to test the performance of the DHT, I have implemented a test module that allows to launch either 1 or 4 nodes (with randomly generated keys) and to launch either 1 or 4 test machines. If there is only one test machine, this machine adds 4000 randomly generated keys, and then do a lookup for all of these 4000 keys. If there are 4 test machines, each of these adds 1000 keys and then do a lookup for all of these 1000 keys (all the test machines run concurrently). As my computer has 8 cores, we could consider that all these nodes and test machines are running in parallel as if they were distributed. However, there is no network latency here, so these tests may differ from real distributed tests. We will measure the lookup time for each configuration. In the case of multiple test machines, the lookup times of all machines are added together.
The results are given Table 1. The lookup time is an average and is given in second (not very accurate, but we are only interested here by the difference between the configurations).

\begin{table}[h]
\centering
\begin{tabular}{lcc}
 & 1 node & 4 nodes \\\hline
1 test machine & 0.003 & 0.003\\\hline
4 test machines on the same node & 0.08 & 0.09\\\hline
4 test machines each on a different node & X & 0.002\\\hline
\end{tabular}
\caption{Average lookup time (seconds)}
\label{tab:results}
\end{table}

Thus, we see that the distributed nature of the DHT really improves the performance when the clients (test machines) act concurrently on the system but not all on the same node.

\section{Conclusions}

During this seminar I have understood the basics of the P2P Chord DHT and I have made some tests to highlight how the distributed nature of this DHT affects performance.


\end{document}
